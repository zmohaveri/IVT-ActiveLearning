{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IVT-dispendix-analysis-latest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmohaveri/IVT-ActiveLearning/blob/main/IVT_dispendix_analysis_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpBIlv3zngMO"
      },
      "source": [
        "#import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE-Hdhz7nPE3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib notebook\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import linregress\n",
        "from math import sqrt\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.signal import savgol_filter\n",
        "from collections import OrderedDict \n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime as tm\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from itertools import combinations \n",
        "from string import ascii_uppercase as alphabets\n",
        "\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Odc40zPNqW3l",
        "outputId": "2b6cc911-f113-4de2-a2ae-dadf7c08daa7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "plt.clf()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCPW17ImqaNU"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg_U_JgGniKV"
      },
      "source": [
        "#Import and clean the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JJpius8qiV8"
      },
      "source": [
        "##get all data and build dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxqLWpolUxCy",
        "outputId": "48304a9f-5efd-4386-d4db-4b8fc5adbf0b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YydkwFi70Kk"
      },
      "source": [
        "def extract_slopes(data_path, fitting_range=(0,1900)):\n",
        "  df = pd.read_excel(data_path,header=11).drop(columns='Time [s]')\n",
        "  df['Unnamed: 0'] = df['Unnamed: 0'].astype(str) + df['Unnamed: 1'].astype(str)\n",
        "  df = df.drop(columns='Unnamed: 1')\n",
        "  df.rename(columns={\"Unnamed: 0\": \"well_name\"}, inplace='True')\n",
        "\n",
        "  time = df.keys()[1:].to_numpy()\n",
        "  n_samples = len(df)\n",
        "\n",
        "  fit_df = fit_line_to_range(df,fitting_range,time)\n",
        "\n",
        "  return fit_df\n",
        "\n",
        "\n",
        "def fit_line_to_range(df,fitting_range: tuple,time):\n",
        "  fitting_range_index = (fitting_range[0]//time[1],fitting_range[1]//time[1])\n",
        "  time_interval = time[fitting_range_index[0] : fitting_range_index[1]]\n",
        "  size = len(df)\n",
        "  fit_df = pd.DataFrame(columns=['well_name','slopes','intercepts'])\n",
        "  for i in range(size):\n",
        "    fit = linregress(time_interval.astype(float), df.loc[i][fitting_range_index[0]+1 : fitting_range_index[1]+1].to_numpy().astype(float))\n",
        "    slope = fit[0]#slope\n",
        "    intercept = fit[1]\n",
        "    name = df['well_name'][i]\n",
        "    fit_df.loc[i] = [name, slope, intercept]\n",
        "\n",
        "  return fit_df\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iAxBi21neqw"
      },
      "source": [
        "def build_dataset(main_path='/content/drive/MyDrive/Thesis/IVT/dispendix/data/**',fitting_range=(0,1900)):\n",
        "  dataset_df = pd.DataFrame()   #(columns=['well_name',\t'spermidine',\t'template',\t'mgcl2',\t'rntp','slopes','intercepts'])\n",
        "  for folder in glob.iglob(main_path):\n",
        "    if os.path.isdir(folder): \n",
        "      data_df, scanning_labels  = extract_iteration_data(folder,fitting_range)\n",
        "      dataset_df = dataset_df.append(data_df.drop(columns='well_name'))\n",
        "  data_with_rep = dataset_df.copy()\n",
        "  dataset_df = dataset_df.groupby(scanning_labels).mean()\n",
        "  return dataset_df, data_with_rep\n",
        "\n",
        "def extract_iteration_data(folder_path,fitting_range=(0,1900)):\n",
        "  for filepath in glob.iglob(folder_path+'/*concentrations.csv'):\n",
        "    df = pd.read_csv(filepath)\n",
        "    columns = df.columns.tolist() \n",
        "    cols_to_use = columns[:len(columns)-1] # drop the last column\n",
        "    concentrations_df = pd.read_csv(filepath, usecols=cols_to_use)\n",
        "\n",
        "  for filepath in glob.iglob(folder_path+'/*Measurements.xlsx'):\n",
        "    fit_df = extract_slopes(filepath,fitting_range)\n",
        "    \n",
        "\n",
        "  data_df = pd.merge(concentrations_df, fit_df, on='well_name')\n",
        "  scanning_labels = concentrations_df.keys().drop('well_name').tolist()\n",
        "\n",
        "  return data_df , scanning_labels"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnYyLqdeEM56"
      },
      "source": [
        "dataset_df, data_with_rep = build_dataset()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "iW1dpXPwP8dQ",
        "outputId": "76ef6098-c828-4214-9c61-f9051074e6ed"
      },
      "source": [
        "dataset_df"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>slopes</th>\n",
              "      <th>intercepts</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spermidine</th>\n",
              "      <th>template</th>\n",
              "      <th>mgcl2</th>\n",
              "      <th>rntp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.02152876833142303</th>\n",
              "      <th>95.23207767186518</th>\n",
              "      <th>143.10464180984184</th>\n",
              "      <th>0.9847087691706968</th>\n",
              "      <td>0.002492</td>\n",
              "      <td>682.381818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.062337550982015655</th>\n",
              "      <th>153.91264330762155</th>\n",
              "      <th>78.4861265671817</th>\n",
              "      <th>4.28212905484041</th>\n",
              "      <td>4.713098</td>\n",
              "      <td>-186.809091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.11623389002402451</th>\n",
              "      <th>16.136738422476515</th>\n",
              "      <th>165.70020862077715</th>\n",
              "      <th>2.7098979688052776</th>\n",
              "      <td>-0.017222</td>\n",
              "      <td>651.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.1183816784710423</th>\n",
              "      <th>31.136445856774113</th>\n",
              "      <th>41.473296536343476</th>\n",
              "      <th>2.1230639941471594</th>\n",
              "      <td>3.971077</td>\n",
              "      <td>461.427273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.17560331533827356</th>\n",
              "      <th>193.75605594796642</th>\n",
              "      <th>183.8144310633305</th>\n",
              "      <th>3.1100230728395157</th>\n",
              "      <td>-0.016919</td>\n",
              "      <td>676.754545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.64975547797833</th>\n",
              "      <th>62.32946278285184</th>\n",
              "      <th>56.52436226041098</th>\n",
              "      <th>3.0662172289335548</th>\n",
              "      <td>1.109663</td>\n",
              "      <td>418.372727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.861589921454165</th>\n",
              "      <th>108.14719435549787</th>\n",
              "      <th>10.66563066736862</th>\n",
              "      <th>2.724431365412825</th>\n",
              "      <td>2.714966</td>\n",
              "      <td>903.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.891616974582526</th>\n",
              "      <th>154.21956804045206</th>\n",
              "      <th>108.0123561522101</th>\n",
              "      <th>3.5129162575956836</th>\n",
              "      <td>0.178956</td>\n",
              "      <td>656.445455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.949120696874306</th>\n",
              "      <th>172.0531242979353</th>\n",
              "      <th>82.15719033037426</th>\n",
              "      <th>1.085919271889283</th>\n",
              "      <td>0.112088</td>\n",
              "      <td>656.709091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.988987313150195</th>\n",
              "      <th>142.20481836570056</th>\n",
              "      <th>34.328151525644785</th>\n",
              "      <th>4.7856838375925035</th>\n",
              "      <td>5.715623</td>\n",
              "      <td>313.445455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>192 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                 slopes  intercepts\n",
              "spermidine           template           mgcl2              rntp                                    \n",
              "0.02152876833142303  95.23207767186518  143.10464180984184 0.9847087691706968  0.002492  682.381818\n",
              "0.062337550982015655 153.91264330762155 78.4861265671817   4.28212905484041    4.713098 -186.809091\n",
              "0.11623389002402451  16.136738422476515 165.70020862077715 2.7098979688052776 -0.017222  651.100000\n",
              "0.1183816784710423   31.136445856774113 41.473296536343476 2.1230639941471594  3.971077  461.427273\n",
              "0.17560331533827356  193.75605594796642 183.8144310633305  3.1100230728395157 -0.016919  676.754545\n",
              "...                                                                                 ...         ...\n",
              "9.64975547797833     62.32946278285184  56.52436226041098  3.0662172289335548  1.109663  418.372727\n",
              "9.861589921454165    108.14719435549787 10.66563066736862  2.724431365412825   2.714966  903.727273\n",
              "9.891616974582526    154.21956804045206 108.0123561522101  3.5129162575956836  0.178956  656.445455\n",
              "9.949120696874306    172.0531242979353  82.15719033037426  1.085919271889283   0.112088  656.709091\n",
              "9.988987313150195    142.20481836570056 34.328151525644785 4.7856838375925035  5.715623  313.445455\n",
              "\n",
              "[192 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCIizJrokt43",
        "outputId": "58322e2a-02a8-4847-aa47-b19491fb608e"
      },
      "source": [
        "dataset_x = np.array([[float(x) for x in row] for row in dataset_df.index.values]) #This is just to get an array out of df, because index values are returned as tuples of strings instead of list of floats.\n",
        "dataset_x.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWIdt6m4k3JO",
        "outputId": "3f759417-90fe-409d-cebd-ea3c06e23c25"
      },
      "source": [
        "dataset_y = dataset_df['intercepts'].to_numpy()\n",
        "dataset_y.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wsgGTz0yWUR"
      },
      "source": [
        "#NN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBYdXWJxy7o-"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKGneFYWkQFs"
      },
      "source": [
        "def train_model(x_train,y_train,x_test,y_test,batch_size,input_size,layer_sizes,activations,optimizer,epochs,loss='MeanSquaredError'):\n",
        "  assert len(layer_sizes) == len(activations)\n",
        "  n_layers=len(layer_sizes)\n",
        "  model = Sequential([LayerNormalization(input_shape=(input_size,)),\n",
        "                      Dense(layer_sizes[0],activation=activations[0],input_shape=(input_size,),name='layer1')])\n",
        "  for i in range(1,n_layers):\n",
        "    model.add(Dense(layer_sizes[i],activation=activations[i],name=f'layer{i+1}'))\n",
        "  \n",
        "  model.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = loss, #MeanSquaredLogarithmicError\n",
        "  )\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs)\n",
        "  loss_history=history.history['loss']\n",
        "\n",
        "  plt.scatter(x=np.arange(1,epochs+1),y=loss_history)\n",
        "  plt.show()\n",
        "\n",
        "  loss=model.evaluate(x_test,y_test)\n",
        "  print('test loss: ',loss)\n",
        "\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBKeQzv1wiNq",
        "outputId": "4d72c09a-5df2-497c-8681-56fe6a169803"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(dataset_x,dataset_y,test_size=0.2,random_state=100)\n",
        "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(153, 4) (39, 4) (153,) (39,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "niT7V9IqvxlY",
        "outputId": "cc088679-edfa-4d4e-879a-4f9f8c0f5c3b"
      },
      "source": [
        "layer_sizes = [10,6,4,1]\n",
        "activations = ['sigmoid','relu','relu','relu']\n",
        "opt = Adam(learning_rate=0.002)\n",
        "model = train_model(x_train,y_train,x_test,y_test,batch_size=10,input_size=4,layer_sizes=layer_sizes,activations=activations,optimizer=opt,epochs=500)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_normalization_11 (Laye (None, 4)                 8         \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 6)                 66        \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 4)                 28        \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 157\n",
            "Trainable params: 157\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 526687.6875\n",
            "Epoch 2/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 526476.1250\n",
            "Epoch 3/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 526194.3125\n",
            "Epoch 4/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 525836.5625\n",
            "Epoch 5/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 525363.3750\n",
            "Epoch 6/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 524758.6250\n",
            "Epoch 7/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 524024.0625\n",
            "Epoch 8/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 523074.6562\n",
            "Epoch 9/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 521882.4062\n",
            "Epoch 10/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 520353.9375\n",
            "Epoch 11/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 518379.8750\n",
            "Epoch 12/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 515853.0312\n",
            "Epoch 13/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 512725.5312\n",
            "Epoch 14/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 508941.5938\n",
            "Epoch 15/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 504307.7500\n",
            "Epoch 16/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 498929.8750\n",
            "Epoch 17/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 492367.5938\n",
            "Epoch 18/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 485135.4375\n",
            "Epoch 19/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 476664.2500\n",
            "Epoch 20/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 466353.1875\n",
            "Epoch 21/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 454945.8438\n",
            "Epoch 22/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 442858.0938\n",
            "Epoch 23/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 429946.4688\n",
            "Epoch 24/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 416849.6250\n",
            "Epoch 25/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 402288.8750\n",
            "Epoch 26/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 387979.6250\n",
            "Epoch 27/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 374003.9375\n",
            "Epoch 28/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 359584.2188\n",
            "Epoch 29/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 345428.4375\n",
            "Epoch 30/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 332128.4688\n",
            "Epoch 31/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 319547.7500\n",
            "Epoch 32/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 306886.0312\n",
            "Epoch 33/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 295745.3750\n",
            "Epoch 34/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 285362.1562\n",
            "Epoch 35/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 275540.2188\n",
            "Epoch 36/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 266849.3750\n",
            "Epoch 37/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 259242.8438\n",
            "Epoch 38/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 252623.8125\n",
            "Epoch 39/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 246825.1250\n",
            "Epoch 40/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 242235.1406\n",
            "Epoch 41/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 237841.0938\n",
            "Epoch 42/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 234611.5625\n",
            "Epoch 43/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 232541.4375\n",
            "Epoch 44/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 230626.6094\n",
            "Epoch 45/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 229408.6562\n",
            "Epoch 46/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 228438.0938\n",
            "Epoch 47/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 227572.0469\n",
            "Epoch 48/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 226806.8438\n",
            "Epoch 49/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 226244.4688\n",
            "Epoch 50/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 226071.5312\n",
            "Epoch 51/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225679.1406\n",
            "Epoch 52/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225409.7969\n",
            "Epoch 53/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225252.1562\n",
            "Epoch 54/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225219.1094\n",
            "Epoch 55/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225107.4844\n",
            "Epoch 56/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225113.6719\n",
            "Epoch 57/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225232.7031\n",
            "Epoch 58/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224928.0781\n",
            "Epoch 59/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224955.2969\n",
            "Epoch 60/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.0469\n",
            "Epoch 61/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224947.1875\n",
            "Epoch 62/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224881.7969\n",
            "Epoch 63/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224962.2188\n",
            "Epoch 64/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225006.9062\n",
            "Epoch 65/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225112.4375\n",
            "Epoch 66/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224932.3125\n",
            "Epoch 67/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224944.5938\n",
            "Epoch 68/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224951.5000\n",
            "Epoch 69/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225082.8438\n",
            "Epoch 70/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224974.3281\n",
            "Epoch 71/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224955.9688\n",
            "Epoch 72/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224934.8281\n",
            "Epoch 73/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224937.7188\n",
            "Epoch 74/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224890.2969\n",
            "Epoch 75/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225005.8750\n",
            "Epoch 76/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224949.7969\n",
            "Epoch 77/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224953.7031\n",
            "Epoch 78/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225017.4844\n",
            "Epoch 79/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224947.2969\n",
            "Epoch 80/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224920.5938\n",
            "Epoch 81/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225008.9219\n",
            "Epoch 82/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224970.8281\n",
            "Epoch 83/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225055.3750\n",
            "Epoch 84/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224927.6562\n",
            "Epoch 85/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225093.3125\n",
            "Epoch 86/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224927.3750\n",
            "Epoch 87/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225019.1094\n",
            "Epoch 88/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224988.7031\n",
            "Epoch 89/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224923.7188\n",
            "Epoch 90/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224940.1250\n",
            "Epoch 91/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225071.8125\n",
            "Epoch 92/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224932.0000\n",
            "Epoch 93/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224925.3906\n",
            "Epoch 94/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224936.8594\n",
            "Epoch 95/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224986.9219\n",
            "Epoch 96/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224954.0469\n",
            "Epoch 97/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224988.4375\n",
            "Epoch 98/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224895.4219\n",
            "Epoch 99/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224921.3281\n",
            "Epoch 100/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224915.6875\n",
            "Epoch 101/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224946.0938\n",
            "Epoch 102/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224894.2188\n",
            "Epoch 103/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224899.0312\n",
            "Epoch 104/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224883.0312\n",
            "Epoch 105/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225042.5156\n",
            "Epoch 106/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224899.1406\n",
            "Epoch 107/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224955.9531\n",
            "Epoch 108/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224891.6562\n",
            "Epoch 109/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224907.7969\n",
            "Epoch 110/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225016.3438\n",
            "Epoch 111/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225139.3438\n",
            "Epoch 112/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224925.9531\n",
            "Epoch 113/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224999.8438\n",
            "Epoch 114/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224928.7656\n",
            "Epoch 115/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225031.8438\n",
            "Epoch 116/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224982.3750\n",
            "Epoch 117/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225028.5000\n",
            "Epoch 118/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224948.1094\n",
            "Epoch 119/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224954.3594\n",
            "Epoch 120/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224967.8750\n",
            "Epoch 121/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224904.4375\n",
            "Epoch 122/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224927.4219\n",
            "Epoch 123/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224888.8125\n",
            "Epoch 124/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.9375\n",
            "Epoch 125/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224895.1406\n",
            "Epoch 126/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224918.5312\n",
            "Epoch 127/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224902.9531\n",
            "Epoch 128/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224907.8438\n",
            "Epoch 129/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224924.3906\n",
            "Epoch 130/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225049.9375\n",
            "Epoch 131/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224961.7969\n",
            "Epoch 132/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224895.5312\n",
            "Epoch 133/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224908.4219\n",
            "Epoch 134/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224885.0156\n",
            "Epoch 135/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224935.2188\n",
            "Epoch 136/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224910.7500\n",
            "Epoch 137/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224882.2031\n",
            "Epoch 138/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224964.5156\n",
            "Epoch 139/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224935.7344\n",
            "Epoch 140/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224971.8438\n",
            "Epoch 141/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.1719\n",
            "Epoch 142/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224940.3906\n",
            "Epoch 143/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224884.7031\n",
            "Epoch 144/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224985.5625\n",
            "Epoch 145/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224990.5312\n",
            "Epoch 146/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224994.3281\n",
            "Epoch 147/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224924.9375\n",
            "Epoch 148/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224940.5000\n",
            "Epoch 149/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224942.6094\n",
            "Epoch 150/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224916.9219\n",
            "Epoch 151/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224876.2031\n",
            "Epoch 152/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224951.9219\n",
            "Epoch 153/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224928.4375\n",
            "Epoch 154/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224846.1406\n",
            "Epoch 155/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224886.3750\n",
            "Epoch 156/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224932.9688\n",
            "Epoch 157/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225052.1875\n",
            "Epoch 158/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224979.9531\n",
            "Epoch 159/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224969.2500\n",
            "Epoch 160/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224928.5469\n",
            "Epoch 161/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224917.3906\n",
            "Epoch 162/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224931.9219\n",
            "Epoch 163/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224933.5625\n",
            "Epoch 164/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.3594\n",
            "Epoch 165/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.3594\n",
            "Epoch 166/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.1406\n",
            "Epoch 167/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224910.6406\n",
            "Epoch 168/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224924.2031\n",
            "Epoch 169/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224925.5469\n",
            "Epoch 170/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224909.5625\n",
            "Epoch 171/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225009.9531\n",
            "Epoch 172/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224952.6250\n",
            "Epoch 173/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224995.7188\n",
            "Epoch 174/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224974.6094\n",
            "Epoch 175/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224976.0000\n",
            "Epoch 176/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224874.6719\n",
            "Epoch 177/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224938.3750\n",
            "Epoch 178/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.7500\n",
            "Epoch 179/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224935.6406\n",
            "Epoch 180/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225066.6406\n",
            "Epoch 181/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224924.6250\n",
            "Epoch 182/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224908.7031\n",
            "Epoch 183/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224926.0469\n",
            "Epoch 184/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224902.9219\n",
            "Epoch 185/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225005.7500\n",
            "Epoch 186/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224849.2031\n",
            "Epoch 187/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.4062\n",
            "Epoch 188/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224890.1250\n",
            "Epoch 189/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224937.7812\n",
            "Epoch 190/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225063.5781\n",
            "Epoch 191/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224880.6250\n",
            "Epoch 192/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224911.6094\n",
            "Epoch 193/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225036.9375\n",
            "Epoch 194/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224878.7500\n",
            "Epoch 195/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224906.8281\n",
            "Epoch 196/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224867.0000\n",
            "Epoch 197/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224917.2500\n",
            "Epoch 198/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225060.3125\n",
            "Epoch 199/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224874.4375\n",
            "Epoch 200/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224901.8594\n",
            "Epoch 201/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224999.6094\n",
            "Epoch 202/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225147.7656\n",
            "Epoch 203/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224974.4531\n",
            "Epoch 204/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224990.8438\n",
            "Epoch 205/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224890.2500\n",
            "Epoch 206/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225018.3281\n",
            "Epoch 207/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224897.7188\n",
            "Epoch 208/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224870.9219\n",
            "Epoch 209/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224941.7031\n",
            "Epoch 210/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224898.6875\n",
            "Epoch 211/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225023.8438\n",
            "Epoch 212/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224945.4844\n",
            "Epoch 213/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225064.4688\n",
            "Epoch 214/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224882.7188\n",
            "Epoch 215/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.5938\n",
            "Epoch 216/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224946.8438\n",
            "Epoch 217/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224902.4375\n",
            "Epoch 218/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224975.3750\n",
            "Epoch 219/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.2500\n",
            "Epoch 220/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225017.6719\n",
            "Epoch 221/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224995.4844\n",
            "Epoch 222/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225318.0938\n",
            "Epoch 223/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224992.7812\n",
            "Epoch 224/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225059.5625\n",
            "Epoch 225/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225005.1562\n",
            "Epoch 226/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225109.4062\n",
            "Epoch 227/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224854.5938\n",
            "Epoch 228/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224937.3281\n",
            "Epoch 229/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224971.2969\n",
            "Epoch 230/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225000.3438\n",
            "Epoch 231/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224971.0625\n",
            "Epoch 232/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224859.6406\n",
            "Epoch 233/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224969.5938\n",
            "Epoch 234/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224940.9219\n",
            "Epoch 235/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224939.0000\n",
            "Epoch 236/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224977.0000\n",
            "Epoch 237/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224950.2969\n",
            "Epoch 238/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224936.9219\n",
            "Epoch 239/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224953.7188\n",
            "Epoch 240/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224916.4219\n",
            "Epoch 241/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224922.3281\n",
            "Epoch 242/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224931.5625\n",
            "Epoch 243/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224942.1719\n",
            "Epoch 244/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224911.6562\n",
            "Epoch 245/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225106.7500\n",
            "Epoch 246/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225060.0469\n",
            "Epoch 247/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224979.5781\n",
            "Epoch 248/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224887.1094\n",
            "Epoch 249/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224890.1406\n",
            "Epoch 250/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224942.6094\n",
            "Epoch 251/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224885.7031\n",
            "Epoch 252/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224873.2344\n",
            "Epoch 253/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224933.0781\n",
            "Epoch 254/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224922.9531\n",
            "Epoch 255/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224903.8125\n",
            "Epoch 256/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224880.9219\n",
            "Epoch 257/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224976.7344\n",
            "Epoch 258/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224935.6094\n",
            "Epoch 259/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224928.7344\n",
            "Epoch 260/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224927.8438\n",
            "Epoch 261/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224902.3750\n",
            "Epoch 262/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224920.6562\n",
            "Epoch 263/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.4844\n",
            "Epoch 264/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224967.1094\n",
            "Epoch 265/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224916.9688\n",
            "Epoch 266/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224906.0156\n",
            "Epoch 267/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224934.9531\n",
            "Epoch 268/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225069.0156\n",
            "Epoch 269/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225038.4062\n",
            "Epoch 270/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224949.9844\n",
            "Epoch 271/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224893.7812\n",
            "Epoch 272/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224919.0625\n",
            "Epoch 273/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225164.7031\n",
            "Epoch 274/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224912.9219\n",
            "Epoch 275/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224949.4375\n",
            "Epoch 276/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224885.6250\n",
            "Epoch 277/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224899.0625\n",
            "Epoch 278/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224936.5000\n",
            "Epoch 279/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224909.5156\n",
            "Epoch 280/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224927.8750\n",
            "Epoch 281/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224997.0156\n",
            "Epoch 282/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224976.1875\n",
            "Epoch 283/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224952.7812\n",
            "Epoch 284/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225235.0781\n",
            "Epoch 285/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225037.3906\n",
            "Epoch 286/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225017.8750\n",
            "Epoch 287/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225002.8281\n",
            "Epoch 288/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224972.4375\n",
            "Epoch 289/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225003.7188\n",
            "Epoch 290/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225043.0625\n",
            "Epoch 291/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224911.8125\n",
            "Epoch 292/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224960.5781\n",
            "Epoch 293/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224974.5625\n",
            "Epoch 294/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224984.7031\n",
            "Epoch 295/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224907.2344\n",
            "Epoch 296/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224913.4062\n",
            "Epoch 297/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225042.5938\n",
            "Epoch 298/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224867.0312\n",
            "Epoch 299/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224966.5156\n",
            "Epoch 300/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224978.0156\n",
            "Epoch 301/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225137.3281\n",
            "Epoch 302/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225013.8594\n",
            "Epoch 303/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224937.2812\n",
            "Epoch 304/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224961.1719\n",
            "Epoch 305/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225000.5781\n",
            "Epoch 306/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225303.9531\n",
            "Epoch 307/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224973.2812\n",
            "Epoch 308/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224983.7656\n",
            "Epoch 309/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224935.3750\n",
            "Epoch 310/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224951.3438\n",
            "Epoch 311/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224948.9375\n",
            "Epoch 312/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.0938\n",
            "Epoch 313/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224906.6719\n",
            "Epoch 314/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225017.8281\n",
            "Epoch 315/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224911.5781\n",
            "Epoch 316/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224951.3125\n",
            "Epoch 317/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224992.4375\n",
            "Epoch 318/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224928.7344\n",
            "Epoch 319/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225000.8906\n",
            "Epoch 320/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224873.7500\n",
            "Epoch 321/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224914.4531\n",
            "Epoch 322/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224926.1719\n",
            "Epoch 323/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224995.6406\n",
            "Epoch 324/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224859.4062\n",
            "Epoch 325/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224944.9375\n",
            "Epoch 326/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224910.7188\n",
            "Epoch 327/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224947.8438\n",
            "Epoch 328/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224946.7656\n",
            "Epoch 329/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224829.5469\n",
            "Epoch 330/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224956.7031\n",
            "Epoch 331/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224970.8438\n",
            "Epoch 332/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224890.1406\n",
            "Epoch 333/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224876.5469\n",
            "Epoch 334/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224903.2188\n",
            "Epoch 335/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224935.6562\n",
            "Epoch 336/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225004.3438\n",
            "Epoch 337/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225100.8125\n",
            "Epoch 338/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224886.2500\n",
            "Epoch 339/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225052.5781\n",
            "Epoch 340/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224842.1719\n",
            "Epoch 341/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.3281\n",
            "Epoch 342/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224956.7344\n",
            "Epoch 343/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224935.6562\n",
            "Epoch 344/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225019.2188\n",
            "Epoch 345/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224928.5938\n",
            "Epoch 346/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224972.1250\n",
            "Epoch 347/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224919.4531\n",
            "Epoch 348/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224888.1250\n",
            "Epoch 349/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224889.1562\n",
            "Epoch 350/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224946.2969\n",
            "Epoch 351/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225188.7812\n",
            "Epoch 352/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224891.4844\n",
            "Epoch 353/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224931.4219\n",
            "Epoch 354/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224959.9688\n",
            "Epoch 355/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224910.4062\n",
            "Epoch 356/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224972.1875\n",
            "Epoch 357/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224945.5156\n",
            "Epoch 358/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225043.6875\n",
            "Epoch 359/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224990.6719\n",
            "Epoch 360/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225073.8281\n",
            "Epoch 361/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224940.3125\n",
            "Epoch 362/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224935.2656\n",
            "Epoch 363/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224884.6250\n",
            "Epoch 364/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224903.3750\n",
            "Epoch 365/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224886.3281\n",
            "Epoch 366/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224963.1406\n",
            "Epoch 367/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224933.9062\n",
            "Epoch 368/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224960.1562\n",
            "Epoch 369/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224927.1094\n",
            "Epoch 370/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224938.4844\n",
            "Epoch 371/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225047.9688\n",
            "Epoch 372/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224870.4375\n",
            "Epoch 373/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224938.3750\n",
            "Epoch 374/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224933.1562\n",
            "Epoch 375/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224920.8906\n",
            "Epoch 376/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224975.1094\n",
            "Epoch 377/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224983.8438\n",
            "Epoch 378/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224965.0781\n",
            "Epoch 379/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224954.6719\n",
            "Epoch 380/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224929.6406\n",
            "Epoch 381/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224968.0469\n",
            "Epoch 382/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224941.5469\n",
            "Epoch 383/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224951.5312\n",
            "Epoch 384/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224936.7812\n",
            "Epoch 385/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224898.6406\n",
            "Epoch 386/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224910.3750\n",
            "Epoch 387/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224983.8906\n",
            "Epoch 388/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224930.0625\n",
            "Epoch 389/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224880.2656\n",
            "Epoch 390/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224962.2812\n",
            "Epoch 391/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224883.8750\n",
            "Epoch 392/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225299.9531\n",
            "Epoch 393/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224996.2344\n",
            "Epoch 394/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224918.3281\n",
            "Epoch 395/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224887.7656\n",
            "Epoch 396/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225028.5000\n",
            "Epoch 397/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225046.7188\n",
            "Epoch 398/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224928.7812\n",
            "Epoch 399/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225176.6562\n",
            "Epoch 400/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224812.3594\n",
            "Epoch 401/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224947.4219\n",
            "Epoch 402/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224927.9219\n",
            "Epoch 403/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225009.2344\n",
            "Epoch 404/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224895.1094\n",
            "Epoch 405/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224924.6875\n",
            "Epoch 406/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224892.5938\n",
            "Epoch 407/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224890.8750\n",
            "Epoch 408/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224823.4531\n",
            "Epoch 409/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224975.4062\n",
            "Epoch 410/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224969.4062\n",
            "Epoch 411/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224926.0938\n",
            "Epoch 412/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224920.8594\n",
            "Epoch 413/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224894.6719\n",
            "Epoch 414/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224905.9531\n",
            "Epoch 415/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224918.5938\n",
            "Epoch 416/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224934.1250\n",
            "Epoch 417/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224906.7500\n",
            "Epoch 418/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225031.5781\n",
            "Epoch 419/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224901.6719\n",
            "Epoch 420/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224961.5625\n",
            "Epoch 421/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224939.4844\n",
            "Epoch 422/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224942.4062\n",
            "Epoch 423/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224981.5469\n",
            "Epoch 424/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225140.6875\n",
            "Epoch 425/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224900.8438\n",
            "Epoch 426/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224919.8906\n",
            "Epoch 427/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224935.6406\n",
            "Epoch 428/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224964.8125\n",
            "Epoch 429/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224913.2344\n",
            "Epoch 430/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224878.2500\n",
            "Epoch 431/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224914.8438\n",
            "Epoch 432/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224844.0469\n",
            "Epoch 433/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224899.0625\n",
            "Epoch 434/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224924.1250\n",
            "Epoch 435/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224903.9219\n",
            "Epoch 436/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224926.4375\n",
            "Epoch 437/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224945.9844\n",
            "Epoch 438/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224932.2656\n",
            "Epoch 439/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224969.7812\n",
            "Epoch 440/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224953.4375\n",
            "Epoch 441/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225128.0781\n",
            "Epoch 442/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225017.0469\n",
            "Epoch 443/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224895.6094\n",
            "Epoch 444/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225161.4844\n",
            "Epoch 445/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225144.3906\n",
            "Epoch 446/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224914.1250\n",
            "Epoch 447/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225010.4531\n",
            "Epoch 448/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224945.4844\n",
            "Epoch 449/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224920.8906\n",
            "Epoch 450/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224964.6250\n",
            "Epoch 451/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224946.3594\n",
            "Epoch 452/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224915.1562\n",
            "Epoch 453/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224927.5625\n",
            "Epoch 454/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224925.2500\n",
            "Epoch 455/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224915.9531\n",
            "Epoch 456/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224934.0938\n",
            "Epoch 457/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224896.2656\n",
            "Epoch 458/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224871.6406\n",
            "Epoch 459/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224890.1250\n",
            "Epoch 460/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224881.5469\n",
            "Epoch 461/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224879.9688\n",
            "Epoch 462/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224997.4844\n",
            "Epoch 463/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225067.0625\n",
            "Epoch 464/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225020.0000\n",
            "Epoch 465/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224875.6875\n",
            "Epoch 466/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 225176.9375\n",
            "Epoch 467/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224922.0156\n",
            "Epoch 468/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224895.6875\n",
            "Epoch 469/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224892.9688\n",
            "Epoch 470/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224852.0000\n",
            "Epoch 471/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225133.2812\n",
            "Epoch 472/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224891.3750\n",
            "Epoch 473/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225080.5469\n",
            "Epoch 474/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224954.7656\n",
            "Epoch 475/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224984.8594\n",
            "Epoch 476/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224929.1719\n",
            "Epoch 477/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224963.8125\n",
            "Epoch 478/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224970.4844\n",
            "Epoch 479/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224929.6406\n",
            "Epoch 480/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224892.5156\n",
            "Epoch 481/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224940.5000\n",
            "Epoch 482/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224866.4844\n",
            "Epoch 483/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224961.6719\n",
            "Epoch 484/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 225007.7344\n",
            "Epoch 485/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224964.8594\n",
            "Epoch 486/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224903.8906\n",
            "Epoch 487/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224973.1719\n",
            "Epoch 488/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224919.3438\n",
            "Epoch 489/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224907.4531\n",
            "Epoch 490/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224912.2344\n",
            "Epoch 491/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224932.7812\n",
            "Epoch 492/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224865.4062\n",
            "Epoch 493/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224908.1875\n",
            "Epoch 494/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224938.0469\n",
            "Epoch 495/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224972.3125\n",
            "Epoch 496/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224882.0938\n",
            "Epoch 497/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224896.6562\n",
            "Epoch 498/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224926.9844\n",
            "Epoch 499/500\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224927.9688\n",
            "Epoch 500/500\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 224958.5312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 3ms/step - loss: 221940.9688\n",
            "test loss:  221940.96875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nHuTeA_M_nu"
      },
      "source": [
        "#Using the trained model to find the maximum Yield"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOhCta8QNBg2"
      },
      "source": [
        "##Generating random grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWHqNx87LjGm"
      },
      "source": [
        "def random_points_in_range(n,ranges):\n",
        "  points = np.empty((n,len(ranges)))\n",
        "  for i,element in enumerate(ranges):\n",
        "    start=min(element[1],element[0])\n",
        "    interval=abs(element[1]-element[0])\n",
        "    rand_check = np.random.rand(n)\n",
        "    randoms = ( rand_check*interval ) + start\n",
        "    points[:,i] = randoms.T\n",
        "  return points"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEisgSX3feLX"
      },
      "source": [
        "def run_n_times(n):\n",
        "  arr=np.empty((n,4))\n",
        "  for i in range(n):\n",
        "    random_grid=random_points_in_range(10000,[(0,100),(0,1000),(0,1000),(0,5)])\n",
        "    preds=model.predict(random_grid)\n",
        "    preds_h=np.array(list(preds))\n",
        "    max_array_index=np.argmax(preds)\n",
        "    max=preds[max_array_index]\n",
        "    max_index=random_grid[max_array_index]\n",
        "    arr[i]= max_index\n",
        "    print(max,max_index)\n",
        "  print('avg:\\n',np.average(arr,axis=0))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7PCaixjg3Vb",
        "outputId": "de724fc5-f917-4e8c-ea7f-b4b8cc9426cf"
      },
      "source": [
        "run_n_times(30)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[547.78217] [ 88.23409833 254.95107055 266.19221109   0.66074759]\n",
            "[547.78217] [6.25411763e+01 1.96659338e+02 2.09541581e+02 1.95396736e-01]\n",
            "[547.78217] [ 86.32072233 166.13694091 140.35466402   4.63958461]\n",
            "[547.78217] [ 73.49567691 239.27945228 256.64638642   0.91498926]\n",
            "[547.78217] [ 79.63173318 294.75221904 222.26629566   1.32662314]\n",
            "[547.78217] [ 95.00187663 233.5068914  210.87437874   4.61147725]\n",
            "[547.78217] [ 96.11361177 235.08222488 243.863096     0.97742737]\n",
            "[547.78217] [ 98.7008991  302.81640633 206.47987881   0.75293812]\n",
            "[547.78217] [ 66.47481055 252.74286367 223.61882674   3.68260808]\n",
            "[547.78217] [4.90081577e+01 1.60634988e+02 1.55354489e+02 3.75689839e-02]\n",
            "[547.78217] [ 58.17309015 115.08200687 141.00752056   3.87872845]\n",
            "[547.78217] [ 79.78335068 113.50603387 133.64178202   3.39214146]\n",
            "[547.78217] [ 94.06602892 193.4043276  170.62063108   4.37666824]\n",
            "[547.78217] [ 58.3307757  194.95102863 130.81435335   2.66254543]\n",
            "[547.78217] [ 67.77769942  81.88506708 101.14514679   3.2607688 ]\n",
            "[547.78217] [ 86.48273221 238.1033079  157.27454833   4.31967701]\n",
            "[547.78217] [ 72.14001792 222.14349178 230.48992243   1.53396075]\n",
            "[547.78217] [ 65.58882254 231.97018397 195.88317275   2.85017975]\n",
            "[547.78217] [ 79.84069961 235.69967896 171.99930894   1.09917263]\n",
            "[547.78217] [ 89.99211813 307.62190664 248.42112255   1.35272124]\n",
            "[547.78217] [ 93.5538396  189.41584907 151.89405011   1.21949281]\n",
            "[547.78217] [ 90.94742594 340.87097003 299.01464273   1.63091513]\n",
            "[547.78217] [ 85.7469797  200.5747541  160.09584801   2.52456612]\n",
            "[547.78217] [ 73.0038005  133.17640098 100.74136349   2.87480429]\n",
            "[547.78217] [ 74.95761273 210.38989289 209.57753546   4.5319924 ]\n",
            "[547.78217] [ 74.36739207 247.05549656 189.6121414    3.15567601]\n",
            "[547.78217] [ 89.7024688  254.82899841 207.42665077   2.66785222]\n",
            "[547.78217] [8.62824609e+01 2.62438532e+02 1.98458730e+02 8.85926221e-02]\n",
            "[547.78217] [22.98574408 47.87364291 52.14554921  4.4647886 ]\n",
            "[547.78217] [ 85.21782355 130.51694003 121.79145242   1.8388402 ]\n",
            "avg:\n",
            " [ 77.48212153 209.60236348 183.57490933   2.38411484]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLpzFKl5O18o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}